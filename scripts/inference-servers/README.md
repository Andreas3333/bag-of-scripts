# Inference servers

## llama.cpp

The use case for for `run-llama-cpp` is running a local inference sever for a gguf model for use in
a browser or with the `lama-vscode` extension for coding models.

For convenient start up on macos, an example `run-llama-server.shortcut` file is provided. Edit and
or import this file into macos Shortcuts and set your preferred keyboard shortcut to invoke it.
Other wise run the script as a normal bash program.
